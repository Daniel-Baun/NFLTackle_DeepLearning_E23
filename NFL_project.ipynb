{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import packages:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is not available\n"
     ]
    }
   ],
   "source": [
    "#%% check if cuda is available:\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda is available')\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print('cuda is not available')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 100, 100]) torch.Size([4, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "max_field_width = 53.3  # in yards\n",
    "max_field_height = 120  # in yards\n",
    "num_channels = 8  #2 for position, 3 for player attributes, 3 for ball attributes\n",
    "\n",
    "class NFLDataset(Dataset):\n",
    "    def __init__(self, csv_file, grid_size=(50, 50), transform=None):\n",
    "        self.nfl_data = pd.read_csv(csv_file)\n",
    "        self.grid_size = grid_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nfl_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_row = self.nfl_data.iloc[idx]\n",
    "    \n",
    "        # Initialize a grid with multiple channels\n",
    "        grid = np.zeros((num_channels, *self.grid_size))\n",
    "    \n",
    "        # Center of the grid\n",
    "        center_x, center_y = self.grid_size[0] // 2, self.grid_size[1] // 2\n",
    "    \n",
    "        # Normalize and translate player and ball positions\n",
    "        ball_x, ball_y = data_row['ball_x'], data_row['ball_y']\n",
    "        player_x, player_y = data_row['player_x'], data_row['player_y']\n",
    "    \n",
    "        # Normalize positions\n",
    "        norm_player_x = int((player_x - ball_x) / max_field_width * self.grid_size[0]) + center_x\n",
    "        norm_player_y = int((player_y - ball_y) / max_field_height * self.grid_size[1]) + center_y\n",
    "    \n",
    "        # Place player and ball in their respective position layers\n",
    "        if 0 <= norm_player_x < self.grid_size[0] and 0 <= norm_player_y < self.grid_size[1]:\n",
    "            grid[0, norm_player_x, norm_player_y] = 1  # Player position layer\n",
    "            grid[1, center_x, center_y] = 1  # Ball position layer (always at the center)\n",
    "    \n",
    "        # Populate other attribute layers (speed, acceleration, direction) for player and ball\n",
    "        # Normalize these attributes as needed\n",
    "        grid[2, norm_player_x, norm_player_y] = data_row['player_s']  # Player speed\n",
    "        grid[3, norm_player_x, norm_player_y] = data_row['player_a']  # Player acceleration\n",
    "        grid[4, norm_player_x, norm_player_y] = data_row['player_dir']  # Player direction\n",
    "        grid[5, center_x, center_y] = data_row['ball_s']  # Ball speed\n",
    "        grid[6, center_x, center_y] = data_row['ball_a']  # Ball acceleration\n",
    "        grid[7, center_x, center_y] = data_row['ball_dir']  # Ball direction\n",
    "    \n",
    "        grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "    \n",
    "        # Create the target label tensor\n",
    "        label = torch.tensor(data_row['tackle'], dtype=torch.float32)\n",
    "    \n",
    "        # Check if tackle occurred\n",
    "        if label == 1:\n",
    "            # If a tackle occurred, set the label to 1\n",
    "            label = torch.ones((1, 1), dtype=torch.float32)\n",
    "        else:\n",
    "            # If no tackle occurred, set the label to 0\n",
    "            label = torch.zeros((1, 1), dtype=torch.float32)\n",
    "        #print(label)\n",
    "        if self.transform:\n",
    "            grid_tensor = self.transform(grid_tensor)\n",
    "\n",
    "        return grid_tensor, label\n",
    "\n",
    "# Usage example\n",
    "nfl_dataset = NFLDataset(csv_file='data/extended_tackles_with_tracking_full.csv', grid_size=(100, 100))\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(nfl_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Example of iterating over the DataLoader\n",
    "for grid, label in dataloader:\n",
    "    print(grid.shape, label.shape)\n",
    "    break  # Remove this to iterate over the entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([[1.]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rasmu\\OneDrive - Aarhus universitet\\7. Semester\\Deep Learning\\Project\\NFLTackle_DeepLearning_E23\\NFL_project.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m grid, label \u001b[39m=\u001b[39m nfl_dataset[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLabel: \u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m visualize_grid(grid\u001b[39m.\u001b[39;49mnumpy())\n",
      "\u001b[1;32mc:\\Users\\rasmu\\OneDrive - Aarhus universitet\\7. Semester\\Deep Learning\\Project\\NFLTackle_DeepLearning_E23\\NFL_project.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m combined_grid[grid[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m  \u001b[39m# Mark ball positions\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Visualize the combined grid\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(combined_grid, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhot\u001b[39m\u001b[39m'\u001b[39m, interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39mcolorbar()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mCombined Player and Ball Positions\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "def visualize_grid(grid):\n",
    "    # Create a combined grid for visualization\n",
    "    # Assuming channel 0 is player position and channel 1 is ball position\n",
    "    combined_grid = np.zeros_like(grid[0])\n",
    "    combined_grid[grid[0] == 1] = 1  # Mark player positions\n",
    "    combined_grid[grid[1] == 1] = 2  # Mark ball positions\n",
    "\n",
    "    # Visualize the combined grid\n",
    "    plt.imshow(combined_grid, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.title('Combined Player and Ball Positions')\n",
    "    plt.show()\n",
    "\n",
    "# Usage example\n",
    "nfl_dataset = NFLDataset(csv_file='data/extended_tackles_with_tracking_test_batch_200.csv', grid_size=(50, 50))\n",
    "\n",
    "# Test the visualization\n",
    "for i in range(4):  # Visualize the first 3 examples\n",
    "    grid, label = nfl_dataset[i]\n",
    "    print(f\"Label: {label}\")\n",
    "    visualize_grid(grid.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 50 * 50, 128)  # Adjust the input features\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 50 * 50)  # Flatten the output for the fully connected layer\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criteria = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "nfl_dataset_big = NFLDataset(csv_file='data/extended_tackles_with_tracking_full.csv', grid_size=(50, 50))\n",
    "nfl_dataset = NFLDataset(csv_file='data/extended_tackles_with_tracking_test_batch_200.csv', grid_size=(50, 50))\n",
    "\n",
    "# Define the proportions\n",
    "total_size = len(nfl_dataset)  # Use the nfl_dataset instance you already created\n",
    "train_size = int(total_size * 0.7)  # 70% of data\n",
    "val_size = int(total_size * 0.2)  # 20% of data\n",
    "test_size = total_size - train_size - val_size  # Remaining 10% for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(nfl_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 50, 50]) torch.Size([4, 1, 1]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    outputs = model(inputs)\n",
    "    print(inputs.shape, labels.shape, outputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([4, 1])) must be the same as input size (torch.Size([1, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rasmu\\OneDrive - Aarhus universitet\\7. Semester\\Deep Learning\\Project\\NFLTackle_DeepLearning_E23\\NFL_project.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m label \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# Reshape labels to [batch_size, 1]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(outputs, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/NFL_project.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\rasmu\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rasmu\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rasmu\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[0;32m    726\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    727\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[0;32m    728\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\rasmu\\miniconda3\\envs\\deep\\Lib\\site-packages\\torch\\nn\\functional.py:3193\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3190\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m-> 3193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3195\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([4, 1])) must be the same as input size (torch.Size([1, 1]))"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define your model (SimpleCNN) here\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define your loss function (Binary Cross Entropy with logits) and optimizer\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming you have train_loader defined\n",
    "num_epochs = 2  # Change to the desired number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        label = labels.float().view(-1, 1)  # Reshape labels to [batch_size, 1]\n",
    "        loss = loss_function(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homedanielbaunMaster1.SemesterDistributedenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
