<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Define the directory containing the CSV files\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Load the tackles data\n",
    "tackles_file = os.path.join(data_dir, 'filtered_tackles_no_assists.csv')\n",
    "tackles_data = pd.read_csv(tackles_file)\n",
    "\n",
    "# Load the games data\n",
    "games_file = os.path.join(data_dir, 'games.csv')\n",
    "games_data = pd.read_csv(games_file)\n",
    "\n",
    "# Merge the tackles and games data on gameId and playId\n",
    "merged_data = tackles_data.merge(games_data[['gameId', 'week']], on=['gameId'], how='left')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(os.path.join(data_dir, 'tackles_with_week.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.42\n",
      "51.69\n",
      "63.8\n",
      "59.76\n",
      "80.71\n",
      "39.2\n",
      "39.42\n",
      "71.29\n",
      "84.59\n",
      "76.31\n",
      "80.74\n",
      "77.77\n",
      "80.24\n",
      "42.6\n",
      "61.6\n",
      "55.65\n",
      "71.21\n",
      "94.36\n",
      "74.81\n",
      "76.42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Load the tackles data\n",
    "tackles_file = os.path.join(data_dir, 'tackles_with_week.csv')\n",
    "tackles_data = pd.read_csv(tackles_file)\n",
    "\n",
    "# Limit the number of rows to process (first 100)\n",
    "limit = 20\n",
    "\n",
    "# Function to get player info from tracking data\n",
    "def get_player_info(game_id, play_id, nfl_id, week):\n",
    "    # Load the corresponding tracking data for the week\n",
    "    tracking_file = os.path.join(data_dir, f'filtered_tracking_week_{week}.csv')\n",
    "    tracking_data = pd.read_csv(tracking_file)\n",
    "\n",
    "    # Find the rows in tracking_data that match the playId, gameId, and nflId for the player\n",
    "    player_info = tracking_data[(tracking_data['playId'] == play_id) &\n",
    "                               (tracking_data['gameId'] == game_id) &\n",
    "                               (tracking_data['nflId'] == int(nfl_id))]\n",
    "\n",
    "    if player_info.empty:\n",
    "        print(f'No tracking data found for player {nfl_id} in play {play_id} of game {game_id} in week {week}')\n",
    "        player_info = {'x': 0, 'y': 0, 'a': 0, 's': 0, 'dir': 0}\n",
    "        return None\n",
    "\n",
    "    # Get the 19th last row or the last available row if there are fewer rows\n",
    "    player_info = player_info.iloc[-min(19, len(player_info))]\n",
    "    return player_info\n",
    "# Add player info to tackles data (even if player_info is None)\n",
    "for index, row in tackles_data.iterrows():\n",
    "    if index >= limit:\n",
    "        break   # Stop processing after reaching the limit\n",
    "\n",
    "    game_id = row['gameId']\n",
    "    play_id = row['playId']\n",
    "    nfl_id = row['nflId']\n",
    "    week = row['week']\n",
    "\n",
    "    player_info = get_player_info(game_id, play_id, nfl_id, week)\n",
    "    print(player_info['x'])\n",
    "    row['player_x'] = player_info['x']\n",
    "    row['player_y'] = player_info['y']\n",
    "    row['player_a'] = player_info['a']\n",
    "    row['player_s'] = player_info['s']\n",
    "    row['player_dir'] = player_info['dir']\n",
    "\n",
    "# Save the updated tackles data to a new CSV file\n",
    "tackles_data.to_csv(os.path.join(data_dir, 'tackles_with_info.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 1 of 20\n",
      "Processing row 2 of 20\n",
      "Processing row 3 of 20\n",
      "Processing row 4 of 20\n",
      "Processing row 5 of 20\n",
      "Processing row 6 of 20\n",
      "Processing row 7 of 20\n",
      "Processing row 8 of 20\n",
      "Processing row 9 of 20\n",
      "Processing row 10 of 20\n",
      "Processing row 11 of 20\n",
      "Processing row 12 of 20\n",
      "Processing row 13 of 20\n",
      "Processing row 14 of 20\n",
      "Processing row 15 of 20\n",
      "Processing row 16 of 20\n",
      "Processing row 17 of 20\n",
      "Processing row 18 of 20\n",
      "Processing row 19 of 20\n",
      "Processing row 20 of 20\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the main file, limiting to the first 20 rows\n",
    "tackles_df = pd.read_csv('data/tackles_with_week.csv', nrows=20)\n",
    "\n",
    "# Get the total number of rows for progress tracking\n",
    "total_rows = len(tackles_df)\n",
    "\n",
    "# Prepare a list to hold the extended rows\n",
    "extended_rows = []\n",
    "\n",
    "# Iterate through each row in the tackles file\n",
    "for index, row in tackles_df.iterrows():\n",
    "    # Print progress\n",
    "    print(f\"Processing row {index + 1} of {total_rows}\")\n",
    "\n",
    "    # Construct the filename for the tracking data\n",
    "    tracking_filename = f\"data/filtered_tracking_week_{row['week']}.csv\"\n",
    "    \n",
    "    # Read the tracking data for the corresponding week\n",
    "    tracking_df = pd.read_csv(tracking_filename)\n",
    "    \n",
    "    # Filter the tracking data to find the matching rows\n",
    "    matching_rows = tracking_df[\n",
    "        (tracking_df['gameId'] == row['gameId']) &\n",
    "        (tracking_df['playId'] == row['playId']) &\n",
    "        (tracking_df['nflId'] == row['nflId'])\n",
    "    ]\n",
    "    \n",
    "    # Check if there are enough rows to extract the 19th last one\n",
    "    if len(matching_rows) >= 19:\n",
    "        # Get the 19th last row\n",
    "        selected_row = matching_rows.iloc[-19]\n",
    "        \n",
    "        # Extract the required columns\n",
    "        extended_data = {\n",
    "            'x': selected_row['x'],\n",
    "            'y': selected_row['y'],\n",
    "            's': selected_row['s'],\n",
    "            'a': selected_row['a'],\n",
    "            'dir': selected_row['dir']\n",
    "        }\n",
    "        \n",
    "        # Append the extended data to the current row\n",
    "        extended_row = {**row.to_dict(), **extended_data}\n",
    "        extended_rows.append(extended_row)\n",
    "\n",
    "# Create a DataFrame from the extended rows\n",
    "extended_df = pd.DataFrame(extended_rows)\n",
    "\n",
    "# Write the extended DataFrame to a new CSV file\n",
    "extended_df.to_csv('data/extended_tackles_with_tracking_test_batch.csv', index=False)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ballCarrierId and saved to 'tackles_with_ballCarrierId.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the main file\n",
    "tackles_df = pd.read_csv('data/tackles_with_week.csv')\n",
    "\n",
    "# Read the plays data\n",
    "plays_df = pd.read_csv('data/plays.csv')\n",
    "\n",
    "# Merge the tackles data with the plays data on gameId and playId\n",
    "merged_df = pd.merge(tackles_df, plays_df[['gameId', 'playId', 'ballCarrierId']], on=['gameId', 'playId'], how='left')\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('data/tackles_with_ballCarrierId.csv', index=False)\n",
    "\n",
    "print(\"Added ballCarrierId and saved to 'tackles_with_ballCarrierId.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_tracking_data(tracking_df, gameId, playId, playerId, prefix):\n",
    "    # Filter the tracking data to find the matching rows\n",
    "    matching_rows = tracking_df[\n",
    "        (tracking_df['gameId'] == gameId) &\n",
    "        (tracking_df['playId'] == playId) &\n",
    "        (tracking_df['nflId'] == playerId)\n",
    "    ]\n",
    "    \n",
    "    # Check if there are enough rows to extract the 19th last one\n",
    "    if len(matching_rows) >= 19:\n",
    "        # Get the 19th last row\n",
    "        selected_row = matching_rows.iloc[-19]\n",
    "    else:\n",
    "        # If not enough rows, use zeroes\n",
    "        selected_row = pd.Series([0, 0, 0, 0, 0], index=['x', 'y', 's', 'a', 'dir'])\n",
    "\n",
    "    # Extract the required columns with the given prefix\n",
    "    return {\n",
    "        f'{prefix}_x': selected_row['x'],\n",
    "        f'{prefix}_y': selected_row['y'],\n",
    "        f'{prefix}_s': selected_row['s'],\n",
    "        f'{prefix}_a': selected_row['a'],\n",
    "        f'{prefix}_dir': selected_row['dir']\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test processing complete. Data saved to 'extended_tackles_with_tracking_all.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "def process_row(row):\n",
    "    # Read the tracking data for the corresponding week\n",
    "    tracking_filename = f\"data/tracking_week_{row['week']}.csv\"\n",
    "    tracking_df = pd.read_csv(tracking_filename)\n",
    "\n",
    "    # Get tracking data for nflId\n",
    "    player_data = get_tracking_data(tracking_df, row['gameId'], row['playId'], row['nflId'], 'player')\n",
    "\n",
    "    # Get tracking data for ballCarrierId\n",
    "    ball_data = get_tracking_data(tracking_df, row['gameId'], row['playId'], row['ballCarrierId'], 'ball')\n",
    "\n",
    "    # Combine data\n",
    "    extended_row = {**row, **player_data, **ball_data}\n",
    "    return extended_row\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Read the main files\n",
    "    tackles_df = pd.read_csv('data/tackles_with_week.csv', nrows=200)\n",
    "    plays_df = pd.read_csv('data/plays.csv')\n",
    "\n",
    "    # Merge the tackles data with the plays data on gameId and playId\n",
    "    merged_df = pd.merge(tackles_df, plays_df[['gameId', 'playId', 'ballCarrierId']], on=['gameId', 'playId'], how='left')\n",
    "\n",
    "    # Process rows in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_row, merged_df.to_dict('records')))\n",
    "    \n",
    "    # Create a DataFrame from the extended rows\n",
    "    extended_df = pd.DataFrame(results)\n",
    "\n",
    "    # Write the extended DataFrame to a new CSV file\n",
    "    extended_df.to_csv('data/extended_tackles_with_tracking_test_batch_200.csv', index=False)\n",
    "\n",
    "    print(\"Test processing complete. Data saved to 'extended_tackles_with_tracking_all.csv'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to data/extended_tackles_with_tracking.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_zero_ball_dir_rows(csv_file, output_file=None):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Filter out rows where ball_dir is 0.0\n",
    "    filtered_data = data[data['ball_dir'] != 0.0]\n",
    "\n",
    "    if output_file:\n",
    "        # Save the filtered data to a new CSV file\n",
    "        filtered_data.to_csv(output_file, index=False)\n",
    "        print(f\"Filtered data saved to {output_file}\")\n",
    "    else:\n",
    "        # Return the filtered DataFrame\n",
    "        return filtered_data\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'data/extended_tackles_with_tracking.csv'  # Replace with your CSV file path\n",
    "output_file = 'data/extended_tackles_with_tracking.csv'  # Replace with your desired output file path\n",
    "\n",
    "# To save the filtered data to a new CSV file\n",
    "remove_zero_ball_dir_rows(csv_file, output_file)\n",
    "\n",
    "# Or to get the filtered data as a DataFrame (without saving to a file)\n",
    "# filtered_df = remove_zero_ball_dir_rows(csv_file)\n",
    "# print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Define the directory containing the CSV files\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Load the tackles data\n",
    "tackles_file = os.path.join(data_dir, 'filtered_tackles_no_assists.csv')\n",
    "tackles_data = pd.read_csv(tackles_file)\n",
    "\n",
    "# Load the games data\n",
    "games_file = os.path.join(data_dir, 'games.csv')\n",
    "games_data = pd.read_csv(games_file)\n",
    "\n",
    "# Merge the tackles and games data on gameId and playId\n",
    "merged_data = tackles_data.merge(games_data[['gameId', 'week']], on=['gameId'], how='left')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(os.path.join(data_dir, 'tackles_with_week.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.42\n",
      "51.69\n",
      "63.8\n",
      "59.76\n",
      "80.71\n",
      "39.2\n",
      "39.42\n",
      "71.29\n",
      "84.59\n",
      "76.31\n",
      "80.74\n",
      "77.77\n",
      "80.24\n",
      "42.6\n",
      "61.6\n",
      "55.65\n",
      "71.21\n",
      "94.36\n",
      "74.81\n",
      "76.42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Load the tackles data\n",
    "tackles_file = os.path.join(data_dir, 'tackles_with_week.csv')\n",
    "tackles_data = pd.read_csv(tackles_file)\n",
    "\n",
    "# Limit the number of rows to process (first 100)\n",
    "limit = 20\n",
    "\n",
    "# Function to get player info from tracking data\n",
    "def get_player_info(game_id, play_id, nfl_id, week):\n",
    "    # Load the corresponding tracking data for the week\n",
    "    tracking_file = os.path.join(data_dir, f'filtered_tracking_week_{week}.csv')\n",
    "    tracking_data = pd.read_csv(tracking_file)\n",
    "\n",
    "    # Find the rows in tracking_data that match the playId, gameId, and nflId for the player\n",
    "    player_info = tracking_data[(tracking_data['playId'] == play_id) &\n",
    "                               (tracking_data['gameId'] == game_id) &\n",
    "                               (tracking_data['nflId'] == int(nfl_id))]\n",
    "\n",
    "    if player_info.empty:\n",
    "        print(f'No tracking data found for player {nfl_id} in play {play_id} of game {game_id} in week {week}')\n",
    "        player_info = {'x': 0, 'y': 0, 'a': 0, 's': 0, 'dir': 0}\n",
    "        return None\n",
    "\n",
    "    # Get the 19th last row or the last available row if there are fewer rows\n",
    "    player_info = player_info.iloc[-min(19, len(player_info))]\n",
    "    return player_info\n",
    "# Add player info to tackles data (even if player_info is None)\n",
    "for index, row in tackles_data.iterrows():\n",
    "    if index >= limit:\n",
    "        break   # Stop processing after reaching the limit\n",
    "\n",
    "    game_id = row['gameId']\n",
    "    play_id = row['playId']\n",
    "    nfl_id = row['nflId']\n",
    "    week = row['week']\n",
    "\n",
    "    player_info = get_player_info(game_id, play_id, nfl_id, week)\n",
    "    print(player_info['x'])\n",
    "    row['player_x'] = player_info['x']\n",
    "    row['player_y'] = player_info['y']\n",
    "    row['player_a'] = player_info['a']\n",
    "    row['player_s'] = player_info['s']\n",
    "    row['player_dir'] = player_info['dir']\n",
    "\n",
    "# Save the updated tackles data to a new CSV file\n",
    "tackles_data.to_csv(os.path.join(data_dir, 'tackles_with_info.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 1 of 20\n",
      "Processing row 2 of 20\n",
      "Processing row 3 of 20\n",
      "Processing row 4 of 20\n",
      "Processing row 5 of 20\n",
      "Processing row 6 of 20\n",
      "Processing row 7 of 20\n",
      "Processing row 8 of 20\n",
      "Processing row 9 of 20\n",
      "Processing row 10 of 20\n",
      "Processing row 11 of 20\n",
      "Processing row 12 of 20\n",
      "Processing row 13 of 20\n",
      "Processing row 14 of 20\n",
      "Processing row 15 of 20\n",
      "Processing row 16 of 20\n",
      "Processing row 17 of 20\n",
      "Processing row 18 of 20\n",
      "Processing row 19 of 20\n",
      "Processing row 20 of 20\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the main file, limiting to the first 20 rows\n",
    "tackles_df = pd.read_csv('data/tackles_with_week.csv', nrows=20)\n",
    "\n",
    "# Get the total number of rows for progress tracking\n",
    "total_rows = len(tackles_df)\n",
    "\n",
    "# Prepare a list to hold the extended rows\n",
    "extended_rows = []\n",
    "\n",
    "# Iterate through each row in the tackles file\n",
    "for index, row in tackles_df.iterrows():\n",
    "    # Print progress\n",
    "    print(f\"Processing row {index + 1} of {total_rows}\")\n",
    "\n",
    "    # Construct the filename for the tracking data\n",
    "    tracking_filename = f\"data/filtered_tracking_week_{row['week']}.csv\"\n",
    "    \n",
    "    # Read the tracking data for the corresponding week\n",
    "    tracking_df = pd.read_csv(tracking_filename)\n",
    "    \n",
    "    # Filter the tracking data to find the matching rows\n",
    "    matching_rows = tracking_df[\n",
    "        (tracking_df['gameId'] == row['gameId']) &\n",
    "        (tracking_df['playId'] == row['playId']) &\n",
    "        (tracking_df['nflId'] == row['nflId'])\n",
    "    ]\n",
    "    \n",
    "    # Check if there are enough rows to extract the 19th last one\n",
    "    if len(matching_rows) >= 19:\n",
    "        # Get the 19th last row\n",
    "        selected_row = matching_rows.iloc[-19]\n",
    "        \n",
    "        # Extract the required columns\n",
    "        extended_data = {\n",
    "            'x': selected_row['x'],\n",
    "            'y': selected_row['y'],\n",
    "            's': selected_row['s'],\n",
    "            'a': selected_row['a'],\n",
    "            'dir': selected_row['dir']\n",
    "        }\n",
    "        \n",
    "        # Append the extended data to the current row\n",
    "        extended_row = {**row.to_dict(), **extended_data}\n",
    "        extended_rows.append(extended_row)\n",
    "\n",
    "# Create a DataFrame from the extended rows\n",
    "extended_df = pd.DataFrame(extended_rows)\n",
    "\n",
    "# Write the extended DataFrame to a new CSV file\n",
    "extended_df.to_csv('data/extended_tackles_with_tracking_test_batch.csv', index=False)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ballCarrierId and saved to 'tackles_with_ballCarrierId.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the main file\n",
    "tackles_df = pd.read_csv('data/tackles_with_week.csv')\n",
    "\n",
    "# Read the plays data\n",
    "plays_df = pd.read_csv('data/plays.csv')\n",
    "\n",
    "# Merge the tackles data with the plays data on gameId and playId\n",
    "merged_df = pd.merge(tackles_df, plays_df[['gameId', 'playId', 'ballCarrierId']], on=['gameId', 'playId'], how='left')\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('data/tackles_with_ballCarrierId.csv', index=False)\n",
    "\n",
    "print(\"Added ballCarrierId and saved to 'tackles_with_ballCarrierId.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 tackles out of 11897\n",
      "Processed 10 tackles out of 11897\n",
      "Processed 20 tackles out of 11897\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rasmu\\OneDrive - Aarhus universitet\\7. Semester\\Deep Learning\\Project\\NFLTackle_DeepLearning_E23\\newSorting.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/newSorting.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m merged_df\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/newSorting.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# Read the tracking data for the corresponding week\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/newSorting.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     tracking_filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/tracking_week_\u001b[39m\u001b[39m{\u001b[39;00mrow[\u001b[39m'\u001b[39m\u001b[39mweek\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/newSorting.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     tracking_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(tracking_filename)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/newSorting.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# Get tracking data for nflId\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rasmu/OneDrive%20-%20Aarhus%20universitet/7.%20Semester/Deep%20Learning/Project/NFLTackle_DeepLearning_E23/newSorting.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     player_data \u001b[39m=\u001b[39m get_tracking_data(tracking_df, row[\u001b[39m'\u001b[39m\u001b[39mgameId\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mplayId\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mnflId\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mplayer\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_tracking_data(tracking_df, gameId, playId, playerId, prefix):\n",
    "    # Filter the tracking data to find the matching rows\n",
    "    matching_rows = tracking_df[\n",
    "        (tracking_df['gameId'] == gameId) &\n",
    "        (tracking_df['playId'] == playId) &\n",
    "        (tracking_df['nflId'] == playerId)\n",
    "    ]\n",
    "    \n",
    "    # Check if there are enough rows to extract the 19th last one\n",
    "    if len(matching_rows) >= 19:\n",
    "        # Get the 19th last row\n",
    "        selected_row = matching_rows.iloc[-19]\n",
    "    else:\n",
    "        # If not enough rows, use zeroes\n",
    "        selected_row = pd.Series([0, 0, 0, 0, 0], index=['x', 'y', 's', 'a', 'dir'])\n",
    "\n",
    "    # Extract the required columns with the given prefix\n",
    "    return {\n",
    "        f'{prefix}_x': selected_row['x'],\n",
    "        f'{prefix}_y': selected_row['y'],\n",
    "        f'{prefix}_s': selected_row['s'],\n",
    "        f'{prefix}_a': selected_row['a'],\n",
    "        f'{prefix}_dir': selected_row['dir']\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test processing complete. Data saved to 'extended_tackles_with_tracking_test_batch.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "def process_row(row):\n",
    "    # Read the tracking data for the corresponding week\n",
    "    tracking_filename = f\"data/tracking_week_{row['week']}.csv\"\n",
    "    tracking_df = pd.read_csv(tracking_filename)\n",
    "\n",
    "    # Get tracking data for nflId\n",
    "    player_data = get_tracking_data(tracking_df, row['gameId'], row['playId'], row['nflId'], 'player')\n",
    "\n",
    "    # Get tracking data for ballCarrierId\n",
    "    ball_data = get_tracking_data(tracking_df, row['gameId'], row['playId'], row['ballCarrierId'], 'ball')\n",
    "\n",
    "    # Combine data\n",
    "    extended_row = {**row, **player_data, **ball_data}\n",
    "    return extended_row\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Read the main files\n",
    "    tackles_df = pd.read_csv('data/tackles_with_week.csv')\n",
    "    plays_df = pd.read_csv('data/plays.csv')\n",
    "\n",
    "    # Merge the tackles data with the plays data on gameId and playId\n",
    "    merged_df = pd.merge(tackles_df, plays_df[['gameId', 'playId', 'ballCarrierId']], on=['gameId', 'playId'], how='left')\n",
    "\n",
    "    # Process rows in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_row, merged_df.to_dict('records')))\n",
    "    \n",
    "    # Create a DataFrame from the extended rows\n",
    "    extended_df = pd.DataFrame(results)\n",
    "\n",
    "    # Write the extended DataFrame to a new CSV file\n",
    "    extended_df.to_csv('data/extended_tackles_with_tracking_test_batch.csv', index=False)\n",
    "\n",
    "    print(\"Test processing complete. Data saved to 'extended_tackles_with_tracking_test_batch.csv'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> 26720c67f449412dbb0741362cb5bc2ef093d531
