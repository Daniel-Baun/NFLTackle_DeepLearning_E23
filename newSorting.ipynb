{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# Define the directory containing the CSV files\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Load the tackles data\n",
    "tackles_file = os.path.join(data_dir, 'filtered_tackles_no_assists.csv')\n",
    "tackles_data = pd.read_csv(tackles_file)\n",
    "\n",
    "# Load the games data\n",
    "games_file = os.path.join(data_dir, 'games.csv')\n",
    "games_data = pd.read_csv(games_file)\n",
    "\n",
    "# Merge the tackles and games data on gameId and playId\n",
    "merged_data = tackles_data.merge(games_data[['gameId', 'week']], on=['gameId'], how='left')\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv(os.path.join(data_dir, 'tackles_with_week.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.42\n",
      "51.69\n",
      "63.8\n",
      "59.76\n",
      "80.71\n",
      "39.2\n",
      "39.42\n",
      "71.29\n",
      "84.59\n",
      "76.31\n",
      "80.74\n",
      "77.77\n",
      "80.24\n",
      "42.6\n",
      "61.6\n",
      "55.65\n",
      "71.21\n",
      "94.36\n",
      "74.81\n",
      "76.42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Load the tackles data\n",
    "tackles_file = os.path.join(data_dir, 'tackles_with_week.csv')\n",
    "tackles_data = pd.read_csv(tackles_file)\n",
    "\n",
    "# Limit the number of rows to process (first 100)\n",
    "limit = 20\n",
    "\n",
    "# Function to get player info from tracking data\n",
    "def get_player_info(game_id, play_id, nfl_id, week):\n",
    "    # Load the corresponding tracking data for the week\n",
    "    tracking_file = os.path.join(data_dir, f'filtered_tracking_week_{week}.csv')\n",
    "    tracking_data = pd.read_csv(tracking_file)\n",
    "\n",
    "    # Find the rows in tracking_data that match the playId, gameId, and nflId for the player\n",
    "    player_info = tracking_data[(tracking_data['playId'] == play_id) &\n",
    "                               (tracking_data['gameId'] == game_id) &\n",
    "                               (tracking_data['nflId'] == int(nfl_id))]\n",
    "\n",
    "    if player_info.empty:\n",
    "        print(f'No tracking data found for player {nfl_id} in play {play_id} of game {game_id} in week {week}')\n",
    "        player_info = {'x': 0, 'y': 0, 'a': 0, 's': 0, 'dir': 0}\n",
    "        return None\n",
    "\n",
    "    # Get the 19th last row or the last available row if there are fewer rows\n",
    "    player_info = player_info.iloc[-min(19, len(player_info))]\n",
    "    return player_info\n",
    "# Add player info to tackles data (even if player_info is None)\n",
    "for index, row in tackles_data.iterrows():\n",
    "    if index >= limit:\n",
    "        break   # Stop processing after reaching the limit\n",
    "\n",
    "    game_id = row['gameId']\n",
    "    play_id = row['playId']\n",
    "    nfl_id = row['nflId']\n",
    "    week = row['week']\n",
    "\n",
    "    player_info = get_player_info(game_id, play_id, nfl_id, week)\n",
    "    print(player_info['x'])\n",
    "    row['player_x'] = player_info['x']\n",
    "    row['player_y'] = player_info['y']\n",
    "    row['player_a'] = player_info['a']\n",
    "    row['player_s'] = player_info['s']\n",
    "    row['player_dir'] = player_info['dir']\n",
    "\n",
    "# Save the updated tackles data to a new CSV file\n",
    "tackles_data.to_csv(os.path.join(data_dir, 'tackles_with_info.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 1 of 20\n",
      "Processing row 2 of 20\n",
      "Processing row 3 of 20\n",
      "Processing row 4 of 20\n",
      "Processing row 5 of 20\n",
      "Processing row 6 of 20\n",
      "Processing row 7 of 20\n",
      "Processing row 8 of 20\n",
      "Processing row 9 of 20\n",
      "Processing row 10 of 20\n",
      "Processing row 11 of 20\n",
      "Processing row 12 of 20\n",
      "Processing row 13 of 20\n",
      "Processing row 14 of 20\n",
      "Processing row 15 of 20\n",
      "Processing row 16 of 20\n",
      "Processing row 17 of 20\n",
      "Processing row 18 of 20\n",
      "Processing row 19 of 20\n",
      "Processing row 20 of 20\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the main file, limiting to the first 20 rows\n",
    "tackles_df = pd.read_csv('data/tackles_with_week.csv', nrows=20)\n",
    "\n",
    "# Get the total number of rows for progress tracking\n",
    "total_rows = len(tackles_df)\n",
    "\n",
    "# Prepare a list to hold the extended rows\n",
    "extended_rows = []\n",
    "\n",
    "# Iterate through each row in the tackles file\n",
    "for index, row in tackles_df.iterrows():\n",
    "    # Print progress\n",
    "    print(f\"Processing row {index + 1} of {total_rows}\")\n",
    "\n",
    "    # Construct the filename for the tracking data\n",
    "    tracking_filename = f\"data/filtered_tracking_week_{row['week']}.csv\"\n",
    "    \n",
    "    # Read the tracking data for the corresponding week\n",
    "    tracking_df = pd.read_csv(tracking_filename)\n",
    "    \n",
    "    # Filter the tracking data to find the matching rows\n",
    "    matching_rows = tracking_df[\n",
    "        (tracking_df['gameId'] == row['gameId']) &\n",
    "        (tracking_df['playId'] == row['playId']) &\n",
    "        (tracking_df['nflId'] == row['nflId'])\n",
    "    ]\n",
    "    \n",
    "    # Check if there are enough rows to extract the 19th last one\n",
    "    if len(matching_rows) >= 19:\n",
    "        # Get the 19th last row\n",
    "        selected_row = matching_rows.iloc[-19]\n",
    "        \n",
    "        # Extract the required columns\n",
    "        extended_data = {\n",
    "            'x': selected_row['x'],\n",
    "            'y': selected_row['y'],\n",
    "            's': selected_row['s'],\n",
    "            'a': selected_row['a'],\n",
    "            'dir': selected_row['dir']\n",
    "        }\n",
    "        \n",
    "        # Append the extended data to the current row\n",
    "        extended_row = {**row.to_dict(), **extended_data}\n",
    "        extended_rows.append(extended_row)\n",
    "\n",
    "# Create a DataFrame from the extended rows\n",
    "extended_df = pd.DataFrame(extended_rows)\n",
    "\n",
    "# Write the extended DataFrame to a new CSV file\n",
    "extended_df.to_csv('data/extended_tackles_with_tracking_test_batch.csv', index=False)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added ballCarrierId and saved to 'tackles_with_ballCarrierId.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the main file\n",
    "tackles_df = pd.read_csv('data/tackles_with_week.csv')\n",
    "\n",
    "# Read the plays data\n",
    "plays_df = pd.read_csv('data/plays.csv')\n",
    "\n",
    "# Merge the tackles data with the plays data on gameId and playId\n",
    "merged_df = pd.merge(tackles_df, plays_df[['gameId', 'playId', 'ballCarrierId']], on=['gameId', 'playId'], how='left')\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('data/tackles_with_ballCarrierId.csv', index=False)\n",
    "\n",
    "print(\"Added ballCarrierId and saved to 'tackles_with_ballCarrierId.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_tracking_data(tracking_df, gameId, playId, playerId, prefix):\n",
    "    # Filter the tracking data to find the matching rows\n",
    "    matching_rows = tracking_df[\n",
    "        (tracking_df['gameId'] == gameId) &\n",
    "        (tracking_df['playId'] == playId) &\n",
    "        (tracking_df['nflId'] == playerId)\n",
    "    ]\n",
    "    \n",
    "    # Check if there are enough rows to extract the 19th last one\n",
    "    if len(matching_rows) >= 19:\n",
    "        # Get the 19th last row\n",
    "        selected_row = matching_rows.iloc[-19]\n",
    "    else:\n",
    "        # If not enough rows, use zeroes\n",
    "        selected_row = pd.Series([0, 0, 0, 0, 0], index=['x', 'y', 's', 'a', 'dir'])\n",
    "\n",
    "    # Extract the required columns with the given prefix\n",
    "    return {\n",
    "        f'{prefix}_x': selected_row['x'],\n",
    "        f'{prefix}_y': selected_row['y'],\n",
    "        f'{prefix}_s': selected_row['s'],\n",
    "        f'{prefix}_a': selected_row['a'],\n",
    "        f'{prefix}_dir': selected_row['dir']\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test processing complete. Data saved to 'extended_tackles_with_tracking_all.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "def process_row(row):\n",
    "    # Read the tracking data for the corresponding week\n",
    "    tracking_filename = f\"data/tracking_week_{row['week']}.csv\"\n",
    "    tracking_df = pd.read_csv(tracking_filename)\n",
    "\n",
    "    # Get tracking data for nflId\n",
    "    player_data = get_tracking_data(tracking_df, row['gameId'], row['playId'], row['nflId'], 'player')\n",
    "\n",
    "    # Get tracking data for ballCarrierId\n",
    "    ball_data = get_tracking_data(tracking_df, row['gameId'], row['playId'], row['ballCarrierId'], 'ball')\n",
    "\n",
    "    # Combine data\n",
    "    extended_row = {**row, **player_data, **ball_data}\n",
    "    return extended_row\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Read the main files\n",
    "    tackles_df = pd.read_csv('data/tackles_with_week.csv', nrows=200)\n",
    "    plays_df = pd.read_csv('data/plays.csv')\n",
    "\n",
    "    # Merge the tackles data with the plays data on gameId and playId\n",
    "    merged_df = pd.merge(tackles_df, plays_df[['gameId', 'playId', 'ballCarrierId']], on=['gameId', 'playId'], how='left')\n",
    "\n",
    "    # Process rows in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_row, merged_df.to_dict('records')))\n",
    "    \n",
    "    # Create a DataFrame from the extended rows\n",
    "    extended_df = pd.DataFrame(results)\n",
    "\n",
    "    # Write the extended DataFrame to a new CSV file\n",
    "    extended_df.to_csv('data/extended_tackles_with_tracking_test_batch_200.csv', index=False)\n",
    "\n",
    "    print(\"Test processing complete. Data saved to 'extended_tackles_with_tracking_all.csv'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to data/extended_tackles_with_tracking.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_zero_ball_dir_rows(csv_file, output_file=None):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Filter out rows where ball_dir is 0.0\n",
    "    filtered_data = data[data['ball_dir'] != 0.0]\n",
    "\n",
    "    if output_file:\n",
    "        # Save the filtered data to a new CSV file\n",
    "        filtered_data.to_csv(output_file, index=False)\n",
    "        print(f\"Filtered data saved to {output_file}\")\n",
    "    else:\n",
    "        # Return the filtered DataFrame\n",
    "        return filtered_data\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'data/extended_tackles_with_tracking.csv'  # Replace with your CSV file path\n",
    "output_file = 'data/extended_tackles_with_tracking.csv'  # Replace with your desired output file path\n",
    "\n",
    "# To save the filtered data to a new CSV file\n",
    "remove_zero_ball_dir_rows(csv_file, output_file)\n",
    "\n",
    "# Or to get the filtered data as a DataFrame (without saving to a file)\n",
    "# filtered_df = remove_zero_ball_dir_rows(csv_file)\n",
    "# print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of unique players involved in a single play: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/extended_tackles_with_tracking_test_batch_200.csv')\n",
    "\n",
    "# Group by 'gameId' and 'playId', then count unique 'nflId' and 'ballCarrierId'\n",
    "grouped = df.groupby(['gameId', 'playId']).agg({'nflId': pd.Series.nunique, 'ballCarrierId': pd.Series.nunique})\n",
    "\n",
    "# Sum the counts of unique 'nflId' and 'ballCarrierId' for each group\n",
    "grouped['total_unique_players'] = grouped['nflId'] + grouped['ballCarrierId']\n",
    "\n",
    "# Find the maximum count of unique players across all groups\n",
    "max_unique_players = grouped['total_unique_players'].max()\n",
    "\n",
    "print(f\"Maximum number of unique players involved in a single play: {max_unique_players}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file into a Pandas DataFrame with explicit dtype\n",
    "csv_file_path = 'data/extended_tackles_with_tracking_test_batch_200.csv'\n",
    "data = pd.read_csv(csv_file_path, dtype={'tackle': int})\n",
    "\n",
    "#Get unique values from the 'tackle' column\n",
    "unique_tackle_values = data['tackle'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "for value in unique_tackle_values:\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 rows and 100 rows after row 8777 have been merged into 'data/extended_tackles_with_tracking_mixed.csv'.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Function to merge rows from two CSV files\n",
    "def merge_csv(input_file, output_file):\n",
    "    # Initialize lists to store the selected rows\n",
    "    first_100_rows = []\n",
    "    rows_after_8777 = []\n",
    "\n",
    "    with open(input_file, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i < 500:\n",
    "                first_100_rows.append(row)\n",
    "            elif i >= 8777 and i < 9277:\n",
    "                rows_after_8777.append(row)\n",
    "    \n",
    "    # Merge the selected rows\n",
    "    merged_rows = first_100_rows + rows_after_8777\n",
    "\n",
    "    # Write the merged rows to a new CSV file\n",
    "    with open(output_file, 'w', newline='') as new_csvfile:\n",
    "        writer = csv.writer(new_csvfile)\n",
    "        writer.writerows(merged_rows)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file_path = 'data/extended_tackles_with_tracking_full.csv'  # Replace with your input CSV file path\n",
    "output_file_path = 'data/extended_tackles_with_tracking_1000.csv'  # Replace with the desired output CSV file path\n",
    "\n",
    "# Call the merge_csv function\n",
    "merge_csv(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"The first 100 rows and 100 rows after row 8777 have been merged into '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value for each column:\n",
      "gameId: 2022110700.0\n",
      "playId: 5096.0\n",
      "nflId: 55241.0\n",
      "tackle: 1.0\n",
      "assist: 0.0\n",
      "forcedFumble: 1.0\n",
      "pff_missedTackle: 1.0\n",
      "week: 9.0\n",
      "ballCarrierId: 55158.0\n",
      "player_x: 113.4\n",
      "player_y: 56.84\n",
      "player_s: 10.89\n",
      "player_a: 8.35\n",
      "player_dir: 359.98\n",
      "ball_x: 117.25\n",
      "ball_y: 53.17\n",
      "ball_s: 10.48\n",
      "ball_a: 8.63\n",
      "ball_dir: 359.96\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Function to print the maximum value for each column\n",
    "def print_max_per_column(csv_file):\n",
    "    with open(csv_file, 'r', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)  # Read and skip the header row\n",
    "        max_values = [float('-inf')] * len(header)  # Initialize with negative infinity\n",
    "\n",
    "        for row in reader:\n",
    "            for i, value in enumerate(row):\n",
    "                max_values[i] = max(max_values[i], float(value))\n",
    "\n",
    "    print(\"Maximum value for each column:\")\n",
    "    for i, header_value in enumerate(header):\n",
    "        print(f\"{header_value}: {max_values[i]}\")\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = 'data/extended_tackles_with_tracking_full.csv'  # Replace with the actual path to your CSV file\n",
    "\n",
    "# Call the function to print maximum values per column\n",
    "print_max_per_column(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with the largest distance:\n",
      "gameId              2.022103e+09\n",
      "playId              3.507000e+03\n",
      "nflId               4.492500e+04\n",
      "tackle              0.000000e+00\n",
      "assist              0.000000e+00\n",
      "forcedFumble        0.000000e+00\n",
      "pff_missedTackle    1.000000e+00\n",
      "week                8.000000e+00\n",
      "ballCarrierId       4.791100e+04\n",
      "player_x            5.163000e+01\n",
      "player_y            3.317000e+01\n",
      "player_s            6.500000e-01\n",
      "player_a            1.450000e+00\n",
      "player_dir          4.287000e+01\n",
      "ball_x              9.778000e+01\n",
      "ball_y              5.262000e+01\n",
      "ball_s              9.200000e+00\n",
      "ball_a              1.610000e+00\n",
      "ball_dir            9.108000e+01\n",
      "distance            5.008118e+01\n",
      "Name: 9634, dtype: float64\n",
      "Largest distance: 50.08118409143298\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "csv_file_path = 'data/extended_tackles_with_tracking_full.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Calculate the Euclidean distance between (player_x, player_y) and (ball_x, ball_y) for each row\n",
    "df['distance'] = np.sqrt((df['player_x'] - df['ball_x'])**2 + (df['player_y'] - df['ball_y'])**2)\n",
    "\n",
    "# Find the row with the largest distance\n",
    "max_distance_row = df.loc[df['distance'].idxmax()]\n",
    "\n",
    "# Print the row with the largest distance and the distance itself\n",
    "print(\"Row with the largest distance:\")\n",
    "print(max_distance_row)\n",
    "print(f\"Largest distance: {max_distance_row['distance']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
